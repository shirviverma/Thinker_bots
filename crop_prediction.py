# -*- coding: utf-8 -*-
"""crop prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u19M3iUwZtCCVUWH0_9GhGE_UFRmP7Gp
"""

#from google.colab import drive
#drive.mount('/content/drive')

AP = []
MN = []

import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter

#!pip install catboost

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from catboost import CatBoostClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn import metrics
from sklearn.model_selection import cross_val_score

pwd

DF0 = pd.read_csv("/content/drive/MyDrive/prediction2/Crop_recommendation.csv")

DF0.head(n=11)

NF = [a for a in DF0.columns if DF0[a].dtypes != 'O']
print("Numerical Features Count {}".format(len(NF)))

#print(NF)

DF0.isnull().sum()*100/len(DF0)

def RandomSamplingImputation(DF0, variable):
    DF0[variable]=DF0[variable]
    random_sample=DF0[variable].dropna().sample(DF0[variable].isnull().sum(),random_state=0)
    random_sample.index=DF0[DF0[variable].isnull()].index
    DF0.loc[DF0[variable].isnull(),variable]=random_sample

DF = [a for a in NF if len(DF0[a].unique())<25]
CF = [a for a in NF if a not in DF]

#print(DF)

#print(CF)

DF0.isnull().sum()*100/len(DF0)

for a in CF:
    if(DF0[a].isnull().sum()*100/len(DF0))>0:
        DF0[a] =DF0[a].fillna(DF0[a].median())

DF0.isnull().sum()*100/len(DF0)

def Mode_Nan(DF0,variable):
    mode=DF0[variable].value_counts().index[0]
    DF0[variable].fillna(mode,inplace=True)

DF0.isnull().sum()*100/len(DF0)

Cf = [a for a in DF0.columns if a not in NF]

#print(Cf)

#for a in CF:
 #  sns.boxplot(data[a])
 #  plt.title(a)
  #  plt.figure(figsize=(15,15))

DF0.tail(n=11)

DF0.isnull().sum()*100/len(DF0)

DF0['label'].unique()

DF0['label'].value_counts()

corrmat = DF0.corr(method = "spearman")
plt.figure(figsize=(20,20))
b=sns.heatmap(corrmat,annot=True)

for a in CF:
    data=DF0.copy()
    sns.distplot(DF0[a])
    plt.xlabel(a)
    plt.ylabel("Count")
    plt.title(a)
    plt.figure(figsize=(15,15))
    plt.show()

for a in CF:
    data=DF0.copy()
    sns.boxplot(data[a])
    plt.title(a)
    plt.figure(figsize=(15,15))

for a in CF:
    data=DF0.copy()
    data[a].hist(bins=25)
    plt.xlabel(a)
    plt.ylabel("Count")
    plt.title(a)
    plt.show()

for a in CF:
    print(a)
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    DF0[a].hist()
    plt.subplot(1, 2, 2)
    stats.probplot(DF0[a], dist="norm", plot=plt)
    plt.show()

corrmat = DF0.corr(method="pearson")
plt.figure(figsize=(20,20))
b=sns.heatmap(corrmat,annot=True)

def qq_Plots(DF0, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    DF0[variable].hist()
    plt.subplot(1, 2, 2)
    stats.probplot(DF0[variable], dist="norm", plot=plt)
    plt.show()

qq_Plots(DF0,"rainfall")

sns.countplot(DF0["temperature"])

a = DF0[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
Target = DF0['label']
Labels = DF0['label']

X_train, X_test, y_train, y_test = train_test_split(a, Target, test_size =0.2, random_state = 2)

print(len(X_train))

print(len(X_test))

print(len(y_train))

print(len(y_test))

print(X_train)

print(X_test)

print(y_train)

print(y_test)

SVM = SVC(gamma='auto')
SVM.fit(X_train,y_train)

DT = DecisionTreeClassifier(criterion="entropy",random_state=2,max_depth=5)
DT.fit(X_train,y_train)

LR = LogisticRegression(random_state=2)
LR.fit(X_train,y_train)

KNN = KNeighborsClassifier(n_neighbors=3)
KNN.fit(X_train, y_train)

CB = CatBoostClassifier(iterations=3000, eval_metric = "AUC")
CB.fit(X_train, y_train)

GNB = GaussianNB()
GNB.fit(X_train, y_train)

XGB_C = xgb.XGBClassifier()
XGB_C.fit(X_train, y_train)

RF=RandomForestClassifier()
RF.fit(X_train,y_train)

PRED0 = SVM.predict(X_test)
x = metrics.accuracy_score(y_test,PRED0)
AP.append(x)
MN.append('SVM')
print("SVM Accuracy ==>: ", x)
print(classification_report(y_test,PRED0))

PRED1 = DT.predict(X_test)
x = metrics.accuracy_score(y_test,PRED1)
AP.append(x)
MN.append('DT')
print("DT Accuracy ==>", x)
print(classification_report(y_test,PRED1))

PRED2 = LR.predict(X_test)
x = metrics.accuracy_score(y_test,PRED2)
AP.append(x)
MN.append('LR')
print("LR Accuracy ==>: ", x)
print(classification_report(y_test,PRED2))

PRED3 = KNN.predict(X_test)
x = metrics.accuracy_score(y_test,PRED3)
AP.append(x)
MN.append('KNN')
print("KNN Accuracy ==>: ", x)
print(classification_report(y_test,PRED3))

PRED4 = CB.predict(X_test)
x = metrics.accuracy_score(y_test,PRED4)
AP.append(x)
MN.append('CB')
print("CB Accuracy ==>", x)
print(classification_report(y_test,PRED4))

PRED5 = GNB.predict(X_test)
x = metrics.accuracy_score(y_test,PRED5)
AP.append(x)
MN.append('GNB')
print("GNB Accuracy ==>: ", x)
print(classification_report(y_test,PRED5))

PRED6 = XGB_C.predict(X_test)
x = metrics.accuracy_score(y_test,PRED6)
AP.append(x)
MN.append('XGB_C')
print("XGB_C Accuracy ==>: ", x)
print(classification_report(y_test,PRED6))

PRED7 = RF.predict(X_test)
x = metrics.accuracy_score(y_test,PRED7)
AP.append(x)
MN.append('RF')
print("RF Accuracy ==>: ", x)
print(classification_report(y_test,PRED7))

O = cross_val_score(SVM,a,Target,cv=5)
O

O = cross_val_score(DT,a,Target,cv=5)
O

O = cross_val_score(LR,a,Target,cv=5)
O

O = cross_val_score(KNN,a,Target,cv=5)
O

O = cross_val_score(GNB,a,Target,cv=5)
O

O = cross_val_score(XGB_C,a,Target,cv=5)
O

O = cross_val_score(RF,a,Target,cv=5)
O

plt.figure(figsize=[10,5],dpi = 100)
plt.title('Compare Accuracy of all Algorithms')
plt.xlabel('Accuracy')
plt.ylabel('Algorithms')
sns.barplot(x = AP,y = MN,palette='dark')

AOM = dict(zip(MN, AP))
for i, j in AOM.items():
    print (i,'==>',j)

q = np.array([[ 22, 67, 78 , 17.1 , 14.42 , 6.2 , 72.3 ]])

PRED00 = RF.predict(q)
print(PRED00)

w = np.array([[22,67,78,17.1,14.42,6.2,72.3]])

PERD000 = RF.predict(w).

print(PERD000)